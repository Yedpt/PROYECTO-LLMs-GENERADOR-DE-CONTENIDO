{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d38a1dc",
   "metadata": {},
   "source": [
    "# Notebook 04 ‚Äì RAG cient√≠fico con arXiv y embeddings\n",
    "\n",
    "En este notebook se implementa una arquitectura RAG (Retrieval-Augmented Generation)\n",
    "para mejorar la calidad del contenido cient√≠fico divulgativo.\n",
    "\n",
    "La informaci√≥n se obtiene de art√≠culos cient√≠ficos de arXiv\n",
    "y se utiliza como contexto para el modelo de lenguaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fe720",
   "metadata": {},
   "source": [
    "## Arquitectura RAG en este proyecto\n",
    "\n",
    "1. Loader: obtiene documentos de arXiv\n",
    "2. Splitter: divide los textos en fragmentos\n",
    "3. Embeddings: convierte texto en vectores\n",
    "4. Vector Store: almacena los vectores\n",
    "5. Retriever: recupera contexto relevante\n",
    "6. LLM: genera la respuesta final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3d0b2",
   "metadata": {},
   "source": [
    "!pip install arxiv langchain-community chromadb tiktoken  || instalamos las dependencias (done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8dd95",
   "metadata": {},
   "source": [
    "(Cargar documentos de arXiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498b2f2",
   "metadata": {},
   "source": [
    "üìÅ backend/app/domain/rag/arxiv_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"quantum computing\",\n",
    "    max_docs=5\n",
    ")\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da3589",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© dividir los documentos?\n",
    "\n",
    "Los LLMs no manejan textos largos de forma eficiente.\n",
    "Dividir los documentos permite:\n",
    "- Mejor b√∫squeda sem√°ntica\n",
    "- Menos ruido\n",
    "- Respuestas m√°s precisas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031723c3",
   "metadata": {},
   "source": [
    "üìÅ backend/app/domain/rag/text_splitter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf889f9",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Un embedding es una representaci√≥n num√©rica de un texto.\n",
    "Permite comparar textos por significado, no por palabras exactas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e52731",
   "metadata": {},
   "source": [
    "C√≥digo (Embeddings con Groq-compatible)\n",
    "\n",
    "‚ö†Ô∏è Groq NO genera embeddings, as√≠ que usamos HuggingFace local/gratuito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d48143",
   "metadata": {},
   "source": [
    "üìÅ backend/app/domain/rag/embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feddf49c",
   "metadata": {},
   "source": [
    "üìÅ backend/app/domain/rag/vectorstore.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./data/vectorstore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9d3c6",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "El retriever se encarga de buscar los fragmentos m√°s relevantes\n",
    "para una pregunta concreta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6012b",
   "metadata": {},
   "source": [
    "üìÅ backend/app/services/rag_service.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "query = \"Explica la computaci√≥n cu√°ntica para el p√∫blico general\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "final_prompt = f\"\"\"\n",
    "Usa el siguiente contexto cient√≠fico para responder de forma divulgativa:\n",
    "\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(final_prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a5665",
   "metadata": {},
   "source": [
    "Este notebook implementa una arquitectura RAG completa\n",
    "para generaci√≥n de contenido cient√≠fico divulgativo,\n",
    "mejorando la calidad, precisi√≥n y fiabilidad del contenido generado.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
