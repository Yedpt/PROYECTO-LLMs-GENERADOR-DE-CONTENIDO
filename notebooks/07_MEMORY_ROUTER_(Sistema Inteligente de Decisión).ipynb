{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6ae90c",
   "metadata": {},
   "source": [
    "# Notebook 07 â€“ Memory + Router Agent\n",
    "\n",
    "Objetivo:\n",
    "Crear un sistema inteligente que:\n",
    "- Recuerde contexto\n",
    "- Decida quÃ© pipeline usar\n",
    "- Orqueste RAG, Graph RAG y Multiagentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f46aaf",
   "metadata": {},
   "source": [
    "ðŸŸ© CELDA 2 â€” MEMORIA (TEORÃA)\n",
    "Tipos de memoria\n",
    "\n",
    "1ï¸âƒ£ Short-term memory\n",
    "\n",
    "Ãšltimos mensajes\n",
    "\n",
    "ConversaciÃ³n actual\n",
    "\n",
    "2ï¸âƒ£ Long-term memory\n",
    "\n",
    "Vector DB\n",
    "\n",
    "Usuarios\n",
    "\n",
    "Historial\n",
    "\n",
    "ðŸ‘‰ AquÃ­ empezamos solo con short-term (bien hecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35dc54",
   "metadata": {},
   "source": [
    "ðŸ“backend/app/models/agents/memory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationMemory:\n",
    "    def __init__(self, max_messages: int = 5):\n",
    "        self.max_messages = max_messages\n",
    "        self.messages = []\n",
    "\n",
    "    def add(self, role: str, content: str):\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "        if len(self.messages) > self.max_messages:\n",
    "            self.messages.pop(0)\n",
    "\n",
    "    def get_context(self) -> str:\n",
    "        return \"\\n\".join(\n",
    "            f\"{m['role']}: {m['content']}\" for m in self.messages\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b82c1",
   "metadata": {},
   "source": [
    "ðŸ§  QuÃ© es esto de arriba\n",
    "\n",
    "Memoria FIFO\n",
    "\n",
    "No DB\n",
    "\n",
    "Perfecta para empezar\n",
    "\n",
    "100% controlable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef7fce",
   "metadata": {},
   "source": [
    "ðŸ“backend/app/models/agents/router_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf95214",
   "metadata": {},
   "source": [
    "ðŸ” REEMPLAZA TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1652f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from app.models.llms.groq_llm import get_groq_llm\n",
    "\n",
    "llm = get_groq_llm()\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Eres un sistema experto que decide quÃ© estrategia usar.\n",
    "\n",
    "Opciones:\n",
    "- rag\n",
    "- graph_rag\n",
    "- multiagent\n",
    "\n",
    "Decide SOLO una opciÃ³n.\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "def route(question: str) -> str:\n",
    "    response = llm.invoke(router_prompt.format(question=question))\n",
    "    decision = response.content.lower()\n",
    "\n",
    "    if \"graph\" in decision:\n",
    "        return \"graph_rag\"\n",
    "    if \"multi\" in decision:\n",
    "        return \"multiagent\"\n",
    "    return \"rag\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821a4cd",
   "metadata": {},
   "source": [
    "ðŸ“backend/app/services/content_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5804cb",
   "metadata": {},
   "source": [
    "ðŸ” REEMPLAZA EL CONTENIDO por esto (si ya tenÃ­a lÃ³gica simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.models.agents.router_agent import route\n",
    "from app.models.agents.memory import ConversationMemory\n",
    "from app.services.rag_service import rag_answer, graph_rag_answer\n",
    "from app.models.agents.marketing_agent import marketing_agent\n",
    "\n",
    "memory = ConversationMemory()\n",
    "\n",
    "def generate_content(question: str) -> str:\n",
    "    memory.add(\"user\", question)\n",
    "\n",
    "    strategy = route(question)\n",
    "\n",
    "    if strategy == \"graph_rag\":\n",
    "        answer = graph_rag_answer(\n",
    "            text=question,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "    elif strategy == \"multiagent\":\n",
    "        answer = marketing_agent(question)\n",
    "\n",
    "    else:\n",
    "        answer = rag_answer(question)\n",
    "\n",
    "    memory.add(\"assistant\", answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18957a7",
   "metadata": {},
   "source": [
    "ðŸŸ© CELDA  â€” CONTROLADOR (NO CAMBIA MUCHO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd8ec9",
   "metadata": {},
   "source": [
    "ðŸ‘‰ Tu content_controller.py ya funciona\n",
    "ðŸ‘‰ Solo asegÃºrate de que llama a generate_content\n",
    "\n",
    "Si ya lo hace â†’ âœ… no toques nada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d8e06",
   "metadata": {},
   "source": [
    "ðŸ§  HE HECHO (MUY IMPORTANTE)\n",
    "\n",
    "He creado:\n",
    "\n",
    "âœ… Memoria conversacional\n",
    "âœ… Router inteligente\n",
    "âœ… OrquestaciÃ³n de pipelines\n",
    "âœ… Sistema adaptable\n",
    "âœ… Base para producciÃ³n\n",
    "\n",
    "Esto es exactamente lo que usan:\n",
    "\n",
    "copilots\n",
    "\n",
    "chatbots avanzados\n",
    "\n",
    "asistentes cientÃ­ficos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
