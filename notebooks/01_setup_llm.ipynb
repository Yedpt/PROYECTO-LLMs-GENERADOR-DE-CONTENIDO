{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdc23d7",
   "metadata": {},
   "source": [
    "# Notebook 01 – Setup inicial del LLM con Groq\n",
    "\n",
    "En este notebook se realiza la configuración inicial del entorno\n",
    "y se prueba el uso de un modelo de lenguaje (LLM) utilizando\n",
    "LangChain y el proveedor Groq.\n",
    "\n",
    "Este notebook tiene como objetivo validar que el sistema puede\n",
    "generar texto antes de integrarlo en un backend con FastAPI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcce1b",
   "metadata": {},
   "source": [
    "instalar dependencias : pip install langchain langchain-community fastapi uvicorn pydantic ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e3f94",
   "metadata": {},
   "source": [
    "## ¿Por qué Groq?\n",
    "\n",
    "Groq es un proveedor de inferencia que permite usar modelos LLaMA\n",
    "de forma gratuita (con límites), muy rápida y sin necesidad de\n",
    "instalar modelos en local.\n",
    "\n",
    "En este proyecto se prioriza:\n",
    "- Bajo coste\n",
    "- Facilidad de uso\n",
    "- Arquitectura desacoplada\n",
    "\n",
    "Por ello, Groq es una opción adecuada para esta prueba de concepto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6e283c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Configurar codificación UTF-8 en Windows\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7a942",
   "metadata": {},
   "source": [
    "## ¿Qué es LangChain?\n",
    "\n",
    "LangChain es un framework que facilita la creación de aplicaciones\n",
    "basadas en modelos de lenguaje.\n",
    "\n",
    "Nos permite:\n",
    "- Cambiar de proveedor de LLM sin reescribir todo el código\n",
    "- Construir prompts dinámicos\n",
    "- Integrar RAG, agentes y herramientas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e273e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Modelo actualizado\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02af1372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API Key cargada (longitud: 56)\n",
      "✓ API Key no contiene caracteres especiales\n"
     ]
    }
   ],
   "source": [
    "# Verificar que la API key se cargó correctamente\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"✓ API Key cargada (longitud: {len(api_key)})\")\n",
    "    # Verificar si tiene caracteres no-ASCII\n",
    "    try:\n",
    "        api_key.encode('ascii')\n",
    "        print(\"✓ API Key no contiene caracteres especiales\")\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"⚠️ API Key contiene caracteres no-ASCII - esto causará problemas\")\n",
    "else:\n",
    "    print(\"✗ API Key NO encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9775f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Un LLM (Large Language Model) es un tipo de modelo de aprendizaje automático que está diseñado para procesar y generar texto de manera similar a como lo haría un humano.\n",
      "\n",
      "**¿Qué es un modelo de aprendizaje automático?**\n",
      "\n",
      "Un modelo de aprendizaje automático es un programa de computadora que aprende a realizar tareas automáticamente, sin necesidad de programación explícita. Esto se logra mediante la utilización de datos de entrenamiento, que se utilizan para ajustar y mejorar el rendimiento del modelo.\n",
      "\n",
      "**¿Cómo funciona un LLM?**\n",
      "\n",
      "Un LLM funciona como sigue:\n",
      "\n",
      "1. **Entrenamiento**: El modelo se entrena con una gran cantidad de texto, que se utiliza para aprender las reglas y patrones lingüísticos.\n",
      "2. **Preprocesamiento**: El texto se preprocesa para eliminar características irrelevantes y prepararlo para ser procesado por el modelo.\n",
      "3. **Generación de texto**: El modelo utiliza las reglas y patrones aprendidos durante el entrenamiento para generar texto nuevo.\n",
      "4. **Evaluación**: El texto generado se evalúa para determinar su calidad y relevancia.\n",
      "\n",
      "**Características clave de un LLM**\n",
      "\n",
      "* **Gran capacidad de procesamiento**: Los LLM pueden procesar grandes cantidades de texto de manera eficiente.\n",
      "* **Capacidad de generación**: Los LLM pueden generar texto nuevo, similar a como lo haría un humano.\n",
      "* **Aprendizaje de patrones**: Los LLM pueden aprender patrones y reglas lingüísticas a partir de datos de entrenamiento.\n",
      "\n",
      "**Ejemplos de aplicaciones de LLM**\n",
      "\n",
      "* **Chatbots**: Los LLM pueden utilizarse para desarrollar chatbots que puedan responder a preguntas y realizar tareas de manera autónoma.\n",
      "* **Traducción automática**: Los LLM pueden utilizarse para traducir texto de manera automática.\n",
      "* **Generación de contenido**: Los LLM pueden utilizarse para generar contenido, como artículos, noticias, etc.\n",
      "\n",
      "**Desafíos y limitaciones de los LLM**\n",
      "\n",
      "* **Calidad del texto generado**: El texto generado por un LLM puede no ser siempre de alta calidad o relevante.\n",
      "* **Limitaciones en la comprensión del contexto**: Los LLM pueden no entender el contexto de un texto de manera óptima.\n",
      "* **Riesgo de propagación de información falsa**: Los LLM pueden propagar información falsa si son entrenados con datos inexactos.\n",
      "\n",
      "Espero que esto te haya ayudado a entender qué es un LLM y cómo funciona. ¡Si tienes más preguntas, no dudes en preguntar!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "    \"Explicame que es un LLM como si fuera un estudiante junior de programacion\"\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e6d0c",
   "metadata": {},
   "source": [
    "Este notebook valida la correcta integración de un LLM real\n",
    "en el entorno de desarrollo.\n",
    "\n",
    "A partir de este punto, el modelo se utilizará para:\n",
    "- Prompt Engineering\n",
    "- Generación de contenido\n",
    "- Backend API\n",
    "- RAG y agentes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
